             CNN-SLAM:实时稠密单目SLAM及深度估计
【摘要】
	鉴于最近深度卷积神经网络(CNN)在深度估计方面的研究有不错的进展，这篇论文研究如何用CNN和单目相机重建出准确且稠密的三维场景。我们提出了一个方法，它可让CNN预测的稠密深度地图与单目SLAM估计的深度图自然融合。该方法在单目SLAM失效的时候（如低纹理区域）仍然能够很好地工作。我们展示了用深度预测来估计重建三维场景的绝对尺度，以解决单目SLAM无法得到绝对尺度这一主要缺陷。最后，我们提出了一个框架，它可将单幅图像中提取的语义标签和稠密SLAM高效地融合在一起，达到从单视角语义一致性地重建场景的效果。在两个数据集上的测试表明我们提出的算法有很高的准确性和很强的鲁棒性。


Migration:
Switching code to OpenCV3 can be done following the official guide at http://docs.opencv.org/master/db/dfa/tutorial_transition_guide.html. 

【引言-部分内容】
	传统SLAM有几点缺陷：
		1.工作范围小。
		2.基于主动感知的SLAM方法无法在日光场景下表现很差。
		3.单目SLAM无法获得绝对尺度，会存在尺度漂移现象。若无法获得绝对尺度，则在VR/AR和机器人应用中都会遇到很大问题。
		4.单目SLAM无法在纯旋转情况下工作，因为此时的stereo-baseline丢失，无法进行立体估计。
	近来，通过机器学习方法基于单幅图像估计场景深度的研究取得了新进展。其中，深度卷积神经网络(CNN)表现出色。其优点是可以得到高分辨的、绝对尺度的深度估计图，甚至在特征点稀疏或repetitive patterns的情况下也表现出色。由于场景的绝对深度可以通过样本训练学习得到，因此不需要基于场景的模型假设，也不需要几何上的约束。然而，此方法的缺陷是虽然深度预测在全局上是准确的，但在深度图中物体的边界处是局部模糊的，这样会丧失物体深度、形状的细节信息。
	我们提出该方法的主要思想是：将深度估计和单目SLAM结合。为了解决深度图中的模糊边缘问题(blurred depth borders)，我们将CNN预测的深度图作为稠密重建的初始猜测(initial guess)，并连续地通过直接法SLAM来优化（依靠small-baseline stereo matching，该方法和参考文献【4】的LSD-SLAM相似，LSD-SLAM是典型的直接法)。非常重要的一点是，small-baseline stereo matching拥有可以改善边缘区域深度估计的潜力。与此同时，CNN预测的深度图有绝对尺度，可为位姿估计提供更多约束条件，显著提高了位姿估计、路径和重建场景的精度。而且由于本方法可克服纯旋转等恶劣情况，因此tracking的稳定性大大增强。另外，该框架可在PC上实时运行，CPU和GPU同时运算，用GPU来执行CNN的深度估计，用CPU来执行深度优化。
	除可用于深度估计之外，CNN还可成功地用于其他高维回归(high-dimension regression)的任务，其中一个典型的例子就是语义分割(semantic segmentation)。我们基于这种方法提出了CNN-SLAM的扩展版本，即用pixel-wise labels将语义labels和稠密SLAM准确一致且高效率地融合在一起。

【相关研究工作】
	SLAM按传感器分可分为基于深度相机的和基于单目相机的，按方法分可分为直接法和特征点法。
	对于单目SLAM，ORB可以说是state-of-the-art的。
	由于深度学习的发展，从单视角进行深度估计的研究得到了越来越多的重视。

【单目语义SLAM】
	我们在该部分介绍所提出的方法，图2显示了该算法的框架和流程。我们使用基于关键帧的SLAM范例，特别的是，我们用参考文献【4 LSD-SLAM】的方法（直接法、半稠密）作为baseline。在该方法中，关键帧作为其子集，从视觉上显著的（visually distinct)帧集合中选出。这些关键帧的位姿将参与全局优化，优化各自的位姿。与此同时，每输入一帧图像，程序就通过该帧与其最近关键帧的transformation来估计该帧的相机位姿。
	为保持算法能以高帧率运行，我们只预测关键帧的深度图（用CNN生成）。特别地，如果当前的位姿估计和现有关键帧相差很多，则基于当前帧生成一个新的关键帧，并用CNN生成其深度图。另外，我们还通过对每个深度预测图进行像素级(pixel-wise)的置信(confidence)衡量，生成一个uncertainty map。由于一般情况下，SLAM用的相机和为产生CNN训练图像而用的相机并不相同，我们提出了将深度图规范化（normalized）的方法，这样对不同内参的相机拍摄的视频流都有较好的鲁棒性。当进行语义label fusion时，我们用另一个卷积网络对输入帧进行语义分割的预测。最后，生成关键帧的位姿图，来全局优化它们的相对位姿。
	该框架很重要的一个环节（同时也是我们的主要贡献），就是通过small-baseline stereo matching来改善CNN预测的关键帧深度图。我们通过在关键帧和对应输入帧之间最小化color consistency来实现。值得一提的是，





















